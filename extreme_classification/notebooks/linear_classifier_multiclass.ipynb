{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pylab as pylab\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class for a simple Linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now there are 9 classes, so 9 logits/output nodes\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2, output_dim=9):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthetic data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_frequencies = [32/80] + [16/80]*2 + [6/80]*2 + [1/80]*4\n",
        "N = 2000\n",
        "\n",
        "np.random.seed(2022)\n",
        "covariance = np.identity(2) * 0.25\n",
        "# A is the most dominant class, {B,C} are next most dominant class, then {D,E} then {F,G,H,I}\n",
        "sampA = np.random.multivariate_normal([0,0], covariance, int(N*class_frequencies[0]))\n",
        "sampB = np.random.multivariate_normal([0,2], covariance, int(N*class_frequencies[1]))\n",
        "sampC = np.random.multivariate_normal([0,-2], covariance, int(N*class_frequencies[2]))\n",
        "sampD = np.random.multivariate_normal([2,0], covariance, int(N*class_frequencies[3]))\n",
        "sampE = np.random.multivariate_normal([-2,0], covariance, int(N*class_frequencies[4]))\n",
        "sampF = np.random.multivariate_normal([-2,2], covariance, int(N*class_frequencies[5]))\n",
        "sampG = np.random.multivariate_normal([2,2], covariance, int(N*class_frequencies[6]))\n",
        "sampH = np.random.multivariate_normal([-2,-2], covariance, int(N*class_frequencies[7]))\n",
        "sampI = np.random.multivariate_normal([2,-2], covariance, int(N*class_frequencies[8]))\n",
        "np.random.seed()\n",
        "X_values = np.concatenate((sampA, sampB, sampC, sampD, sampE, sampF, sampG, sampH, sampI), axis=0)\n",
        "y_flat = np.array([])\n",
        "for i in range(9):\n",
        "    y_flat = np.concatenate((y_flat, np.array([i] * int(N*class_frequencies[i]))))\n",
        "\n",
        "index_shuffle = np.arange(len(X_values))\n",
        "np.random.shuffle(index_shuffle)\n",
        "\n",
        "X_values = X_values[index_shuffle]\n",
        "y_flat = y_flat[index_shuffle]\n",
        "\n",
        "X_train = torch.tensor(X_values).type(torch.FloatTensor)\n",
        "y_train = torch.from_numpy(y_flat).view(-1,1).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plots\n",
        "\n",
        "##### This just shows how our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Draw decision boundaries of the Bayes classifier\n",
        "plt.axvline(x = 1, color='k', linestyle=':', label='Bayes classifier')\n",
        "plt.axvline(x = -1, color='k', linestyle=':')\n",
        "plt.axhline(y = 1, color='k', linestyle=':')\n",
        "plt.axhline(y = -1, color='k', linestyle=':')\n",
        "plt.xlim([-4, 4])\n",
        "plt.ylim([-4, 4])\n",
        "# Plot the data\n",
        "plt.plot(sampA[:,0], sampA[:,1], 'cx', label='Dominant')\n",
        "plt.plot(sampB[:,0], sampB[:,1], '.y', label='Medium')\n",
        "plt.plot(sampC[:,0], sampC[:,1], '.y')\n",
        "plt.plot(sampD[:,0], sampD[:,1], '.g', label='Medium')\n",
        "plt.plot(sampE[:,0], sampE[:,1], '.g')\n",
        "plt.plot(sampF[:,0], sampF[:,1], 'sr', label='Rare')\n",
        "plt.plot(sampG[:,0], sampG[:,1], 'sr')\n",
        "plt.plot(sampH[:,0], sampH[:,1], 'sr')\n",
        "plt.plot(sampI[:,0], sampI[:,1], 'sr')\n",
        "# Formatting plot, adds title, axis labels, saves plot etc.\n",
        "plt.title('Generated data by class', fontsize=25)\n",
        "plt.xlabel('x1', fontsize=20)\n",
        "plt.ylabel('x2', fontsize=20)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.legend(loc='upper left', fontsize=15)\n",
        "plt.axis('scaled')\n",
        "plt.axis([-4,4,-4,4])\n",
        "figure = plt.gcf()\n",
        "figure.set_size_inches(8,8)\n",
        "plt.savefig('../data/imbalMultiData', dpi=1500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A mesh for visualising decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates a mesh of ~800^2 points in [-4,4]^2\n",
        "x1_fit = np.linspace(-4,4,800)\n",
        "x2_fit = np.linspace(-4,4,800)\n",
        "\n",
        "mesh_grid = np.meshgrid(x1_fit, x2_fit)\n",
        "\n",
        "X_mesh = np.hstack([mesh_grid[0].reshape(-1, 1), mesh_grid[1].reshape(-1, 1)]).astype(np.float32)\n",
        "X_mesh = torch.from_numpy(X_mesh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Produces scatter plot, with Bayes classifier. Each point on the dense mesh is coloured by the predicted class\n",
        "\n",
        "def mesh_plot(y_pred, title, fileName):\n",
        "    plt.axvline(x = 1, color='k', linestyle=':')\n",
        "    plt.axvline(x = -1, color='k', linestyle=':')\n",
        "    plt.axhline(y = 1, color='k', linestyle=':')\n",
        "    plt.axhline(y = -1, color='k', linestyle=':')\n",
        "    plt.title(title, fontsize=25)\n",
        "    y_pred_colours = [['c','y','y','g','g','r','r','r','r'][i] for i in y_pred.detach().numpy()]\n",
        "    plt.scatter(X_mesh[:,0], X_mesh[:,1], c = y_pred_colours, marker='.')\n",
        "    plt.xlabel('x1', fontsize=20)\n",
        "    plt.ylabel('x2', fontsize=20)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.axis('scaled')\n",
        "    plt.axis([-4,4,-4,4])\n",
        "    figure = plt.gcf()\n",
        "    figure.set_size_inches(8,8)\n",
        "    plt.savefig(fileName, dpi=800)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_orig = LinearClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelTrain(model, optimizer, criterion):\n",
        "    for epoch in range(50000):\n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train.view(-1))\n",
        "        \n",
        "        # backward\n",
        "        loss.backward()\n",
        "\n",
        "        # optimise\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_erm = deepcopy(model_orig)\n",
        "criterion_erm = torch.nn.CrossEntropyLoss()\n",
        "optimizer_erm = torch.optim.SGD(model_erm.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trains model_erm under vanilla ERM with cross-entropy\n",
        "modelTrain(model_erm, optimizer_erm, criterion_erm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_erm = model_erm.forward(X_mesh)\n",
        "y_pred_erm = torch.argmax(y_pred_erm, axis = 1)\n",
        "mesh_plot(y_pred_erm, 'ERM', '../data/imbalMultiVanilla.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear classifier with additive update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adjustments = torch.from_numpy(np.log(class_frequencies)).type(torch.FloatTensor)\n",
        "# modified predictions with additive update\n",
        "y_pred_adjusted = torch.argmax(model_erm.forward(X_mesh) - adjustments, axis = 1)\n",
        "mesh_plot(y_pred_adjusted, 'Additive update', '../data/imbalMultiAdditive.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear classifier with modified loss\n",
        "\n",
        "In-built weighting of CrossEntropyLoss with weight argument in PyTorch. This gives the loss function as in Menon (4); corresponding to 'balancing' the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_w = deepcopy(model_orig)\n",
        "optimizer_w = torch.optim.SGD(model_w.parameters(), lr=0.01)\n",
        "# class-weight the cross-entropy\n",
        "criterion_w = torch.nn.CrossEntropyLoss(weight = torch.tensor(np.reciprocal(class_frequencies)).type(torch.FloatTensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelTrain(model_w, optimizer_w, criterion_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_w = model_w.forward(X_mesh)\n",
        "y_pred_w = torch.argmax(y_pred_w, axis = 1)\n",
        "mesh_plot(y_pred_w, 'Class-weighted ERM', '../data/imbalMultiWeighted.png')"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
