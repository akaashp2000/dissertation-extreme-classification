{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is for generating plots for Chapter 4.2 (Experimental Results, Synthetic Data - Multiclass).\n",
        "\n",
        "It generates plots for decision regions of different linear multiclass classifiers for 2D data, learned from different loss functions and with post-hoc updates to the discriminant functions ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pylab as pylab\n",
        "\n",
        "params = {\n",
        "    \"legend.fontsize\": \"x-large\",\n",
        "    \"axes.labelsize\": \"x-large\",\n",
        "    \"axes.titlesize\": \"x-large\",\n",
        "    \"xtick.labelsize\": \"x-large\",\n",
        "    \"ytick.labelsize\": \"x-large\",\n",
        "}\n",
        "pylab.rcParams.update(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device\")\n",
        "print(\"-\" * 60)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create data directory, if it doesn't exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"../data\"):\n",
        "    os.makedirs(\"../data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_frequencies = [32 / 80] + [16 / 80] * 2 + [6 / 80] * 2 + [1 / 80] * 4\n",
        "N = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(2022)\n",
        "covariance = np.identity(2) * 0.25\n",
        "\n",
        "# A is the most dominant class, {B,C} are next most dominant class, then {D,E} then {F,G,H,I}\n",
        "sampA = np.random.multivariate_normal([0, 0], covariance, int(N * class_frequencies[0]))\n",
        "sampB = np.random.multivariate_normal([0, 2], covariance, int(N * class_frequencies[1]))\n",
        "sampC = np.random.multivariate_normal(\n",
        "    [0, -2], covariance, int(N * class_frequencies[2])\n",
        ")\n",
        "sampD = np.random.multivariate_normal([2, 0], covariance, int(N * class_frequencies[3]))\n",
        "sampE = np.random.multivariate_normal(\n",
        "    [-2, 0], covariance, int(N * class_frequencies[4])\n",
        ")\n",
        "sampF = np.random.multivariate_normal(\n",
        "    [-2, 2], covariance, int(N * class_frequencies[5])\n",
        ")\n",
        "sampG = np.random.multivariate_normal([2, 2], covariance, int(N * class_frequencies[6]))\n",
        "sampH = np.random.multivariate_normal(\n",
        "    [-2, -2], covariance, int(N * class_frequencies[7])\n",
        ")\n",
        "sampI = np.random.multivariate_normal(\n",
        "    [2, -2], covariance, int(N * class_frequencies[8])\n",
        ")\n",
        "\n",
        "X_values = np.concatenate(\n",
        "    (sampA, sampB, sampC, sampD, sampE, sampF, sampG, sampH, sampI), axis=0\n",
        ")\n",
        "y_flat = np.array([])\n",
        "\n",
        "for i in range(9):\n",
        "    y_flat = np.concatenate((y_flat, np.array([i] * int(N * class_frequencies[i]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed()\n",
        "index_shuffle = np.arange(len(X_values))\n",
        "np.random.shuffle(index_shuffle)\n",
        "\n",
        "X_values = X_values[index_shuffle]\n",
        "y_flat = y_flat[index_shuffle]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_values).type(torch.FloatTensor).to(device)\n",
        "y_train = torch.from_numpy(y_flat).view(-1, 1).type(torch.LongTensor).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Draw decision boundaries of the Bayes classifier\n",
        "plt.axvline(x=1, color=\"k\", linestyle=\":\", label=\"Bayes classifier\")\n",
        "plt.axvline(x=-1, color=\"k\", linestyle=\":\")\n",
        "plt.axhline(y=1, color=\"k\", linestyle=\":\")\n",
        "plt.axhline(y=-1, color=\"k\", linestyle=\":\")\n",
        "plt.xlim([-4, 4])\n",
        "plt.ylim([-4, 4])\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(sampA[:, 0], sampA[:, 1], \"cx\", label=\"Dominant\")\n",
        "plt.plot(sampB[:, 0], sampB[:, 1], \".y\", label=\"Medium\")\n",
        "plt.plot(sampC[:, 0], sampC[:, 1], \".y\")\n",
        "plt.plot(sampD[:, 0], sampD[:, 1], \".g\", label=\"Medium\")\n",
        "plt.plot(sampE[:, 0], sampE[:, 1], \".g\")\n",
        "plt.plot(sampF[:, 0], sampF[:, 1], \"sr\", label=\"Rare\")\n",
        "plt.plot(sampG[:, 0], sampG[:, 1], \"sr\")\n",
        "plt.plot(sampH[:, 0], sampH[:, 1], \"sr\")\n",
        "plt.plot(sampI[:, 0], sampI[:, 1], \"sr\")\n",
        "\n",
        "# Formatting plot, adds title, axis labels, saves plot etc.\n",
        "plt.title(\"Generated data by class\", fontsize=25)\n",
        "plt.xlabel(\"x1\", fontsize=20)\n",
        "plt.ylabel(\"x2\", fontsize=20)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.legend(loc=\"upper left\", fontsize=15)\n",
        "plt.axis(\"scaled\")\n",
        "plt.axis([-4, 4, -4, 4])\n",
        "figure = plt.gcf()\n",
        "figure.set_size_inches(8, 8)\n",
        "plt.savefig(\"../data/imbalMultiData\", dpi=1500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now there are 9 classes, so 9 logits/output nodes\n",
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear classifier module.\n",
        "\n",
        "    This class defines a linear classifier with a specified input dimension and output dimension.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): The dimension of the input features.\n",
        "        output_dim (int): The number of output classes.\n",
        "\n",
        "    Attributes:\n",
        "        linear (nn.Linear): The linear transformation layer.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=2, output_dim=9):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the linear classifier.\n",
        "\n",
        "        This method computes the forward pass of the linear classifier.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelTrain(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion,\n",
        "    X_train: torch.Tensor,\n",
        "    y_train: torch.Tensor,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model using a given optimizer and loss criterion.\n",
        "\n",
        "    This function performs training for a specified number of epochs. It includes the forward pass, backward pass, and optimization steps.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be trained.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer used for updating model parameters.\n",
        "        criterion: The loss criterion used to compute the loss.\n",
        "        X_train (torch.Tensor): The input data tensor for training.\n",
        "        y_train (torch.Tensor): The target labels tensor for training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for epoch in range(50000):\n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train.view(-1))\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "\n",
        "        # optimize\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_orig = LinearClassifier().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vanilla ERM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_erm = deepcopy(model_orig)\n",
        "criterion_erm = nn.CrossEntropyLoss()\n",
        "optimizer_erm = torch.optim.SGD(model_erm.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trains model_erm under vanilla ERM with cross-entropy\n",
        "modelTrain(\n",
        "    model=model_erm,\n",
        "    optimizer=optimizer_erm,\n",
        "    criterion=criterion_erm,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weighted ERM\n",
        "\n",
        "The weights are inverse class probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_w = deepcopy(model_orig)\n",
        "optimizer_w = torch.optim.SGD(model_w.parameters(), lr=0.01)\n",
        "# class-weight the cross-entropy\n",
        "criterion_w = nn.CrossEntropyLoss(\n",
        "    weight=torch.tensor(np.reciprocal(class_frequencies))\n",
        "    .type(torch.FloatTensor)\n",
        "    .to(device)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelTrain(\n",
        "    model=model_w,\n",
        "    optimizer=optimizer_w,\n",
        "    criterion=criterion_w,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A mesh for visualising decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates a mesh of ~800^2 points in [-4,4]^2\n",
        "x1_fit = np.linspace(-4, 4, 800)\n",
        "x2_fit = np.linspace(-4, 4, 800)\n",
        "\n",
        "mesh_grid = np.meshgrid(x1_fit, x2_fit)\n",
        "\n",
        "X_mesh = np.hstack([mesh_grid[0].reshape(-1, 1), mesh_grid[1].reshape(-1, 1)]).astype(\n",
        "    np.float32\n",
        ")\n",
        "X_mesh = torch.from_numpy(X_mesh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Produces scatter plot, with Bayes classifier. Each point on the dense mesh is coloured by the predicted class\n",
        "\n",
        "\n",
        "def mesh_plot(y_pred: torch.Tensor, title: str, fileName: str):\n",
        "    \"\"\"Create a plot which shows the decision boundaries by colouring region by predicted class.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): A tensor of class predictions for X_mesh\n",
        "        title (str): Title of the plot\n",
        "        fileName (str): Filename or path to save the file (you need to include an extension, like .png)\n",
        "    \"\"\"\n",
        "    plt.axvline(x=1, color=\"k\", linestyle=\":\")\n",
        "    plt.axvline(x=-1, color=\"k\", linestyle=\":\")\n",
        "    plt.axhline(y=1, color=\"k\", linestyle=\":\")\n",
        "    plt.axhline(y=-1, color=\"k\", linestyle=\":\")\n",
        "    plt.title(title, fontsize=25)\n",
        "    y_pred_colours = [\n",
        "        [\"c\", \"y\", \"y\", \"g\", \"g\", \"r\", \"r\", \"r\", \"r\"][i]\n",
        "        for i in y_pred.cpu().detach().numpy()\n",
        "    ]\n",
        "    plt.scatter(X_mesh[:, 0], X_mesh[:, 1], c=y_pred_colours, marker=\".\")\n",
        "    plt.xlabel(\"x1\", fontsize=20)\n",
        "    plt.ylabel(\"x2\", fontsize=20)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.axis(\"scaled\")\n",
        "    plt.axis([-4, 4, -4, 4])\n",
        "    figure = plt.gcf()\n",
        "    figure.set_size_inches(8, 8)\n",
        "    plt.savefig(fileName, dpi=800)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vanilla ERM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_erm = model_erm.forward(X_mesh.to(device))\n",
        "y_pred_erm = torch.argmax(y_pred_erm, axis=1).to(device)\n",
        "mesh_plot(y_pred_erm, \"ERM\", \"../data/imbalMultiVanilla.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vanilla ERM + additive update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adjustments = (\n",
        "    torch.from_numpy(np.log(class_frequencies)).type(torch.FloatTensor).to(device)\n",
        ")\n",
        "\n",
        "# modified predictions with additive update\n",
        "y_pred_adjusted = torch.argmax(\n",
        "    model_erm.forward(X_mesh.to(device)) - adjustments, axis=1\n",
        ").to(device)\n",
        "\n",
        "mesh_plot(y_pred_adjusted, \"Additive update\", \"../data/imbalMultiAdditive.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weighted ERM\n",
        "\n",
        "In-built weighting of CrossEntropyLoss with weight argument in PyTorch. This gives the loss function as in Menon (4); corresponding to 'balancing' the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_w = model_w.forward(X_mesh.to(device))\n",
        "y_pred_w = torch.argmax(y_pred_w, axis=1)\n",
        "mesh_plot(y_pred_w, \"Class-weighted ERM\", \"../data/imbalMultiWeighted.png\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
