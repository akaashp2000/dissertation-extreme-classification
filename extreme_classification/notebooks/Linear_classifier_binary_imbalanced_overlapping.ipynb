{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'\n",
    "         }\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for simple Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple linear neural network with input dimension 2 \n",
    "# and output dimension 2 (the discriminant function for each class)\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes in a binary linear classifier's weights and biases\n",
    "# and plots the decision boundary line in [-4,4] * [-4,4]\n",
    "\n",
    "def plot_db(wb, modelType='[modelType]', lins='r'):\n",
    "\n",
    "    w0, w1, b0, b1 = wb[0], wb[1], wb[2], wb[3]\n",
    "    x0 = np.linspace(-4,4,100)\n",
    "    x1 = (b1 - b0 - (w0[0] - w1[0]) * x0) / (w0[1] - w1[1])\n",
    "    plt.plot(x0, x1, lins, label=f'{modelType}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nxrS5pU3Tb8"
   },
   "source": [
    "### Synthetic data generation and visualisation\n",
    "\n",
    "10,000 samples - 95% from Gaussian centered at (-1, -1) with covariance I; the remaining 5% from Gaussian centered at (1, 1) with covariance I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9LpO2hYuGvz"
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "class_prob = [0.95, 0.05]\n",
    "class_freq = [int(N * i) for i in class_prob]\n",
    "\n",
    "covariance = np.identity(2) # change to np.identity(2) * 0.04 to get separable data\n",
    "np.random.seed(2022) # fix seed for a reproducable training set\n",
    "sampA = np.random.multivariate_normal([-1,-1], covariance, class_freq[0])\n",
    "sampB = np.random.multivariate_normal([1,1], covariance, class_freq[1])\n",
    "np.random.seed()\n",
    "\n",
    "X_values = np.concatenate((sampA, sampB), axis=0) # N-by-2 data matrix\n",
    "y_flat = np.array([0] * class_freq[0] + [1] * class_freq[1]) # row vector of labels\n",
    "\n",
    "index_shuffle = np.arange(len(X_values))\n",
    "np.random.shuffle(index_shuffle)\n",
    "\n",
    "# shuffle the rows of data matrix and labels\n",
    "\n",
    "X_values = X_values[index_shuffle]\n",
    "y_flat = y_flat[index_shuffle]\n",
    "\n",
    "# convert to Torch tensors\n",
    "\n",
    "X_train = torch.tensor(X_values).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_flat).view(-1,1).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the standard classifier \n",
    "\n",
    "With single layer NN with Cross-Entropy Loss (for binary classification equivalent to a sigmoid classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear neural network with randomly initialised weights and biases\n",
    "# Each trained network begins with the same initialisation, as they all start\n",
    "# as copies of this network\n",
    "\n",
    "model_bin_orig = LinearClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: model\n",
    "# output: weights and biases in an array\n",
    "# optional: additive update to biases or multiplicative update to weight and biases\n",
    "\n",
    "def getwb(model, add=[0,0], mult=[1,1]):\n",
    "    return [model.linear.weight[0].detach().numpy()/mult[0], model.linear.weight[1].detach().numpy()/mult[1], (model.linear.bias[0].detach().numpy()+add[0])/mult[0], (model.linear.bias[1].detach().numpy()+add[1])/mult[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: model, optimizer for parameters for that model, criterion (or loss)\n",
    "# optional: additive update to outputs before passing through loss function\n",
    "# and trains model and updates its parameters\n",
    "\n",
    "def modelTrain(model, optimizer, criterion, adjust=0):\n",
    "    for epoch in range(10000):\n",
    "    \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output + adjust, y_train.view(-1))\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unweighted cross-entropy ERM\n",
    "model_bin = deepcopy(model_bin_orig)\n",
    "optimizer_bin = torch.optim.SGD(model_bin.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin, optimizer_bin, criterion_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss modified\n",
    "\n",
    "Balance the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-weighted cross-entropy ERM\n",
    "model_bin_bal = deepcopy(model_bin_orig)\n",
    "optimizer_bin_bal = torch.optim.SGD(model_bin_bal.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_bal = torch.nn.CrossEntropyLoss(weight = torch.tensor(np.reciprocal(class_prob)).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_bal, optimizer_bin_bal, criterion_bin_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999\n",
    "cui_weights = [(1 - beta) / (1 - beta ** i) for i in class_freq] # inverse effective number for each class\n",
    "cui_weights = [i / sum(cui_weights) for i in cui_weights] # normalise weights\n",
    "cui_weights = torch.tensor(cui_weights).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted cross-entropy with beta=0.999\n",
    "model_bin_cui = deepcopy(model_bin_orig)\n",
    "optimizer_bin_cui = torch.optim.SGD(model_bin_cui.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_cui = torch.nn.CrossEntropyLoss(weight = cui_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_cui, optimizer_bin_cui, criterion_bin_cui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cao's LDAM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAM Loss taken from Cao et. al code and modified\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        delta = 1.0 / np.sqrt(np.sqrt(class_prob))\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        index_float = index.type(torch.FloatTensor)\n",
    "\n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = torch.matmul(self.delta[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "\n",
    "        # computes logits modified by subtracting the margin for each example (determined by the true class) from the each output for it\n",
    "        outputs_m = outputs - batch_m\n",
    "    \n",
    "        # for each example, replace the logit for the true class by the modified one, and keep the others the same\n",
    "        output = torch.where(index, outputs_m, outputs)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Cao's LDAM loss\n",
    "model_bin_cao = deepcopy(model_bin_orig)\n",
    "optimizer_bin_cao = torch.optim.SGD(model_bin_cao.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_cao = LDAMLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_cao, optimizer_bin_cao, criterion_bin_cao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalised Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss with margin log(pi_j) for where j is the negative class\n",
    "\n",
    "class EqLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(EqLoss, self).__init__()\n",
    "        delta = np.log(class_prob)\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = self.delta.repeat(outputs.shape[0],1)\n",
    "\n",
    "        # computes logits modified by adding the margin, log(pi_j), to each logit f_j for each example\n",
    "        outputs_m = outputs + batch_m\n",
    "    \n",
    "        # for each example, replace the logit for the negative classes by the modified one in outputs_m, and keep original logit for true class\n",
    "        output = torch.where(index, outputs, outputs_m)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with equalised loss\n",
    "model_bin_eq = deepcopy(model_bin_orig)\n",
    "optimizer_bin_eq = torch.optim.SGD(model_bin_eq.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_eq = EqLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_eq, optimizer_bin_eq, criterion_bin_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit-Adjusted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with logit-adjusted loss\n",
    "model_bin_la = deepcopy(model_bin_orig)\n",
    "optimizer_bin_la = torch.optim.SGD(model_bin_la.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_la = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the loss, each logit f_j is replaced by f_j + log(pi_j) in the cross-entropy\n",
    "la_margin_mat = torch.Tensor(np.array([np.log(class_prob) for _ in range(N)])).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_la, optimizer_bin_la, criterion_bin_la, adjust=la_margin_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundaries\n",
    "\n",
    "Decision boundaries for each linear classifier (Vanilla ERM, logit-adjusted, reweighted and modified loss).\n",
    "\n",
    "We also include the Bayes classifier decision boundaries (y = - x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the synthetic data\n",
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "# Plot the decision boundaries - can plot boundaries for different models\n",
    "# Plotting logit-adjusted models (multiplicative and additive updates) can be done as follows:\n",
    "\n",
    "# plot_db(getwb(model_bin, mult=class_prob), modelType='Multiplicative update', lins='g')\n",
    "# plot_db(getwb(model_bin, add=-np.log(class_prob)), modelType='Additive update', lins='b')\n",
    "\n",
    "plot_db(getwb(model_bin), modelType='β = 0', lins='r')\n",
    "plot_db(getwb(model_bin_cui), modelType=f'β = {beta}', lins='b')\n",
    "plot_db(getwb(model_bin_bal), modelType='β = 1', lins='g')\n",
    "plot_db([[0,0], [1,1], 0, 0], modelType='Bayes classifier', lins='k--')\n",
    "# Add legend, title, axes labels, general formatting, saving the plot\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Overlapping', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalOverlapWeighted.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit adjusted decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "# Plot decision boundaries for standard model with weights and biases adjusted according to update\n",
    "plot_db(getwb(model_bin), modelType='ERM', lins='r')\n",
    "plot_db(getwb(model_bin, add=-np.log(class_prob)), modelType='Additive update', lins='b')\n",
    "plot_db(getwb(model_bin, mult=class_prob), modelType='Multiplicative update', lins='g')\n",
    "plot_db([[0,0], [1,1], 0, 0], modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Overlapping', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalOverlapUpdate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin adjusted loss decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plot_db(getwb(model_bin), modelType='ERM', lins='r')\n",
    "plot_db(getwb(model_bin_cao), modelType='LDAM', lins='g')\n",
    "plot_db(getwb(model_bin_eq), modelType='Equalised', lins='b')\n",
    "plot_db(getwb(model_bin_la), modelType='Logit-adjusted', lins='purple')\n",
    "plot_db([[0,0], [1,1], 0, 0], modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Overlapping', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalOverlapMargin.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6dc7853e01262b9d99422a25edd5bb7869d5d1bdf17c6d04e3e60055b23b476"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
