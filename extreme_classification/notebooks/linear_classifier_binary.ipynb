{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for generating plots for Chapter 4.1 (Experimental Results, Synthetic Data - Binary).\n",
    "\n",
    "It generates plots for decision boundaries of different linear binary classifiers for 2D data, learned from different loss functions.\n",
    "\n",
    "For both a) separable and b) overlapping and both i) balanced and ii) imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {\n",
    "    \"legend.fontsize\": \"x-large\",\n",
    "    \"axes.labelsize\": \"x-large\",\n",
    "    \"axes.titlesize\": \"x-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\")\n",
    "print(\"-\" * 60)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier\n",
    "\n",
    "A very simple linear neural network with input dimension 2 and output dimension 2 (the discriminant function for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "Plotting function, extract weights and biases from model, training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_db(wb: List[float], modelType: str = \"[modelType]\", lins=\"r\"):\n",
    "    \"\"\"Plots decision boundary line where -4 <= x, y <= 4, given the weights\n",
    "    and biases of the discriminant functions for each class. Options for\n",
    "    plot title and linestyle.\n",
    "\n",
    "    Args:\n",
    "        wb (List[float]): List of weights and biases.\n",
    "        modelType (str, optional): Label for plot. Defaults to '[modelType]'.\n",
    "        lins (str, optional): Linestyle of plot. Defaults to 'r'.\n",
    "    \"\"\"\n",
    "\n",
    "    w0, w1, b0, b1 = wb[0], wb[1], wb[2], wb[3]\n",
    "    x0 = np.linspace(-4, 4, 100)\n",
    "    x1 = (b1 - b0 - (w0[0] - w1[0]) * x0) / (w0[1] - w1[1])\n",
    "    plt.plot(x0, x1, lins, label=f\"{modelType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwb(\n",
    "    model: LinearClassifier, add: List[float] = [0, 0], mult: List[float] = [1, 1]\n",
    "):\n",
    "    \"\"\"Takes in binary LinearClassifier that has been fitted. Returns the weights and\n",
    "    biases of the two classes.\n",
    "\n",
    "    Args:\n",
    "        model (LinearClassifier): Linear Classifier\n",
    "        add (list, optional): Optional post-hoc addition to model's biases. Defaults to [0, 0].\n",
    "        mult (list, optional): Optional post-hoc scaling of each discriminant function. Defaults to [1, 1].\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The weights and biases of the model, post any offsets and scaling.\n",
    "    \"\"\"\n",
    "    _model = model.cpu()\n",
    "\n",
    "    return [\n",
    "        _model.linear.weight[0].detach().numpy() / mult[0],\n",
    "        _model.linear.weight[1].detach().numpy() / mult[1],\n",
    "        (_model.linear.bias[0].detach().numpy() + add[0]) / mult[0],\n",
    "        (_model.linear.bias[1].detach().numpy() + add[1]) / mult[1],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrain(\n",
    "    model, optimizer, criterion, X_train, y_train, adjust: torch.Tensor = None\n",
    "):\n",
    "    \"\"\"Trains a Pytorch model with an optimizer and criterion.\n",
    "    When evaluating the loss, an adjustment can be applied to the output of the model.\n",
    "\n",
    "    Args:\n",
    "        model: Model to be trained.\n",
    "        optimizer: Pytorch optimizer.\n",
    "        criterion: Loss function\n",
    "        X_train: Training X dataset.\n",
    "        y_train: Training y dataset.\n",
    "        adjust (torch.Tensor, optional): Additive update to output of model. Defaults to 0.\n",
    "    \"\"\"\n",
    "    adjust = torch.Tensor([0]).to(device) if adjust is None else adjust\n",
    "    for _ in range(10000):\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output + adjust, y_train.view(-1))\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generation and visualisation\n",
    "\n",
    "10,000 samples\n",
    "\n",
    "$ (100p)\\% $ from Gaussian centered at \n",
    "$\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "-1\n",
    "\\end{bmatrix}$ \n",
    "with covariance $ \\lambda I_2 $ where $ I_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$; \n",
    "\n",
    "the remaining $ (100(1-p))\\% $ from Gaussian centered at $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}$  with covariance $ \\lambda I_2 $.\n",
    "\n",
    "For a) separable, $ \\lambda = 0.04 $, and for b) overlapping, $ \\lambda = 1 $.\n",
    "\n",
    "For i) balanced, $ p = 0.5 $, and for ii) imbalanced, $ p = 0.95 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "for imbalanced, separable in list(itertools.product(*[[True, False]] * 2)):\n",
    "    config = (imbalanced, separable)\n",
    "    Xy_model_dict[config] = {}\n",
    "\n",
    "    Xy_model_dict[config][\"class_prob\"] = [0.95, 0.05] if imbalanced else [0.5, 0.5]\n",
    "    Xy_model_dict[config][\"class_freq\"] = [\n",
    "        int(N * i) for i in Xy_model_dict[config][\"class_prob\"]\n",
    "    ]\n",
    "    λ = 0.04 if separable else 1\n",
    "\n",
    "    covariance = λ * np.identity(2)\n",
    "    np.random.seed(2022)  # fix seed for a reproducable training set\n",
    "    Xy_model_dict[config][\"sampA\"] = np.random.multivariate_normal(\n",
    "        [-1, -1], covariance, Xy_model_dict[config][\"class_freq\"][0]\n",
    "    )\n",
    "    Xy_model_dict[config][\"sampB\"] = np.random.multivariate_normal(\n",
    "        [1, 1], covariance, Xy_model_dict[config][\"class_freq\"][1]\n",
    "    )\n",
    "    np.random.seed()\n",
    "\n",
    "    X_values = np.concatenate(\n",
    "        (Xy_model_dict[config][\"sampA\"], Xy_model_dict[config][\"sampB\"]), axis=0\n",
    "    )  # N-by-2 data matrix\n",
    "    y_flat = np.array(\n",
    "        [0] * Xy_model_dict[config][\"class_freq\"][0]\n",
    "        + [1] * Xy_model_dict[config][\"class_freq\"][1]\n",
    "    )  # row vector of labels\n",
    "\n",
    "    index_shuffle = np.arange(len(X_values))\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    # shuffle the rows of data matrix and labels\n",
    "\n",
    "    X_values = X_values[index_shuffle]\n",
    "    y_flat = y_flat[index_shuffle]\n",
    "\n",
    "    # convert to Torch tensors\n",
    "\n",
    "    X_train = torch.tensor(X_values).type(torch.FloatTensor).to(device)\n",
    "    y_train = torch.from_numpy(y_flat).view(-1, 1).type(torch.LongTensor).to(device)\n",
    "\n",
    "    Xy_model_dict[config] |= {\"X_train\": X_train, \"y_train\": y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_orig = LinearClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard classifier\n",
    "\n",
    "With single layer NN with Cross-Entropy Loss (for binary classification equivalent to a sigmoid classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin = deepcopy(model_bin_orig)\n",
    "    optimizer_bin = torch.optim.SGD(model_bin.parameters(), lr=0.1, weight_decay=0)\n",
    "    criterion_bin = torch.nn.CrossEntropyLoss().to(device)\n",
    "    Xy_model_dict[config][\"trained_model_bin\"] = modelTrain(\n",
    "        model_bin,\n",
    "        optimizer_bin,\n",
    "        criterion_bin,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss modified\n",
    "\n",
    "Balance the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_bal = deepcopy(model_bin_orig)\n",
    "    optimizer_bin_bal = torch.optim.SGD(\n",
    "        model_bin_bal.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "    criterion_bin_bal = torch.nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(np.reciprocal(Xy_model_dict[config][\"class_prob\"])).type(\n",
    "            torch.FloatTensor\n",
    "        )\n",
    "    ).to(device)\n",
    "    Xy_model_dict[config][\"trained_model_bin_bal\"] = modelTrain(\n",
    "        model_bin_bal,\n",
    "        optimizer_bin_bal,\n",
    "        criterion_bin_bal,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    beta = 0.999\n",
    "    cui_weights = [\n",
    "        (1 - beta) / (1 - beta**i) for i in Xy_model_dict[config][\"class_freq\"]\n",
    "    ]  # inverse effective number for each class\n",
    "    cui_weights = [i / sum(cui_weights) for i in cui_weights]  # normalise weights\n",
    "    cui_weights = torch.tensor(cui_weights).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    model_bin_cui = deepcopy(model_bin_orig)\n",
    "    optimizer_bin_cui = torch.optim.SGD(\n",
    "        model_bin_cui.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "    criterion_bin_cui = torch.nn.CrossEntropyLoss(weight=cui_weights).to(device)\n",
    "    Xy_model_dict[config][\"trained_model_bin_cui\"] = modelTrain(\n",
    "        model_bin_cui,\n",
    "        optimizer_bin_cui,\n",
    "        criterion_bin_cui,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cao's LDAM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAM Loss taken from Cao et. al code and modified\n",
    "\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        delta = 1.0 / np.sqrt(np.sqrt(class_prob))\n",
    "        delta = torch.FloatTensor(delta).to(device)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8).to(device)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        index_float = index.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = torch.matmul(self.delta[None, :], index_float.transpose(0, 1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "\n",
    "        # computes logits modified by subtracting the margin for each example (determined by the true class) from the each output for it\n",
    "        outputs_m = outputs - batch_m\n",
    "\n",
    "        # for each example, replace the logit for the true class by the modified one, and keep the others the same\n",
    "        output = torch.where(index, outputs_m, outputs)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_cao = deepcopy(model_bin_orig)\n",
    "    optimizer_bin_cao = torch.optim.SGD(\n",
    "        model_bin_cao.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "    criterion_bin_cao = LDAMLoss(Xy_model_dict[config][\"class_prob\"])\n",
    "    Xy_model_dict[config][\"trained_model_bin_cao\"] = modelTrain(\n",
    "        model_bin_cao,\n",
    "        optimizer_bin_cao,\n",
    "        criterion_bin_cao,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalised Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss with margin log(pi_j) for where j is the negative class\n",
    "\n",
    "\n",
    "class EqLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(EqLoss, self).__init__()\n",
    "        delta = np.log(class_prob)\n",
    "        delta = torch.FloatTensor(delta).to(device)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8).to(device)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "\n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = self.delta.repeat(outputs.shape[0], 1)\n",
    "\n",
    "        # computes logits modified by adding the margin, log(pi_j), to each logit f_j for each example\n",
    "        outputs_m = outputs + batch_m\n",
    "\n",
    "        # for each example, replace the logit for the negative classes by the modified one in outputs_m, and keep original logit for true class\n",
    "        output = torch.where(index, outputs, outputs_m)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_eq = deepcopy(model_bin_orig)\n",
    "    optimizer_bin_eq = torch.optim.SGD(\n",
    "        model_bin_eq.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "    criterion_bin_eq = EqLoss(Xy_model_dict[config][\"class_prob\"])\n",
    "    Xy_model_dict[config][\"trained_model_bin_eq\"] = modelTrain(\n",
    "        model_bin_eq,\n",
    "        optimizer_bin_eq,\n",
    "        criterion_bin_eq,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit-Adjusted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    la_margin_mat = (\n",
    "        torch.Tensor(np.array([np.log(Xy_model_dict[config][\"class_prob\"])] * N))\n",
    "        .type(torch.float32)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    model_bin_la = deepcopy(model_bin_orig)\n",
    "    optimizer_bin_la = torch.optim.SGD(\n",
    "        model_bin_la.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "    criterion_bin_la = torch.nn.CrossEntropyLoss()\n",
    "    Xy_model_dict[config][\"trained_model_bin_la\"] = modelTrain(\n",
    "        model_bin_la,\n",
    "        optimizer_bin_la,\n",
    "        criterion_bin_la,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "        adjust=la_margin_mat,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the decision boundaries\n",
    "\n",
    "Decision boundaries for each linear classifier (Vanilla ERM, logit-adjusted, reweighted and modified loss).\n",
    "\n",
    "We also include the Bayes classifier decision boundaries (y = - x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    imbalanced, separable = config\n",
    "    # Show the synthetic data\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampA\"][:, 0], Xy_model_dict[config][\"sampA\"][:, 1], \"cx\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampB\"][:, 0], Xy_model_dict[config][\"sampB\"][:, 1], \".y\"\n",
    "    )\n",
    "    # Plot the decision boundaries - can plot boundaries for different models\n",
    "    # Plotting logit-adjusted models (multiplicative and additive updates) can be done as follows:\n",
    "\n",
    "    # plot_db(getwb(model_bin, mult=class_prob), modelType='Multiplicative update', lins='g')\n",
    "    # plot_db(getwb(model_bin, add=-np.log(class_prob)), modelType='Additive update', lins='b')\n",
    "\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin\"]),\n",
    "        modelType=\"\\u03B2 = 0\",\n",
    "        lins=\"r\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin_cui\"]),\n",
    "        modelType=f\"\\u03B2 = {beta}\",\n",
    "        lins=\"b\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin_bal\"]),\n",
    "        modelType=\"\\u03B2 = 1\",\n",
    "        lins=\"g\",\n",
    "    )\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "    # Add legend, title, axes labels, general formatting, saving the plot\n",
    "    plt.legend(loc=\"best\", fontsize=15)\n",
    "    plt.title(\n",
    "        f\"Decision boundaries - {'Separable' if separable else 'Overlapping'}\",\n",
    "        fontsize=25,\n",
    "    )\n",
    "    plt.xlabel(\"x1\", fontsize=20)\n",
    "    plt.ylabel(\"x2\", fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.grid()\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis([-4, 4, -4, 4])\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(8, 8)\n",
    "    plt.savefig(\n",
    "        f\"../data/{'imbal' if imbalanced else 'bal'}{'Sep' if separable else 'Overlap'}Weighted.png\",\n",
    "        dpi=1200,\n",
    "    )\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit adjusted decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampA\"][:, 0], Xy_model_dict[config][\"sampA\"][:, 1], \"cx\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampB\"][:, 0], Xy_model_dict[config][\"sampB\"][:, 1], \".y\"\n",
    "    )\n",
    "    # Plot decision boundaries for standard model with weights and biases adjusted according to update\n",
    "    plot_db(\n",
    "        getwb(\n",
    "            Xy_model_dict[config][\"trained_model_bin\"],\n",
    "            mult=Xy_model_dict[config][\"class_prob\"],\n",
    "        ),\n",
    "        modelType=\"Multiplicative update\",\n",
    "        lins=\"g\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(\n",
    "            Xy_model_dict[config][\"trained_model_bin\"],\n",
    "            add=-np.log(Xy_model_dict[config][\"class_prob\"]),\n",
    "        ),\n",
    "        modelType=\"Additive update\",\n",
    "        lins=\"b\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin\"]), modelType=\"ERM\", lins=\"r\"\n",
    "    )\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\n",
    "        f\"Decision boundaries - {'Separable' if separable else 'Overlapping'}\",\n",
    "        fontsize=25,\n",
    "    )\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.grid()\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis([-4, 4, -4, 4])\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(8, 8)\n",
    "    plt.savefig(\n",
    "        f\"../data/{'imbal' if imbalanced else 'bal'}{'Sep' if separable else 'Overlap'}Update.png\",\n",
    "        dpi=1200,\n",
    "    )\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin adjusted loss decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampA\"][:, 0], Xy_model_dict[config][\"sampA\"][:, 1], \"cx\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        Xy_model_dict[config][\"sampB\"][:, 0], Xy_model_dict[config][\"sampB\"][:, 1], \".y\"\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin\"]), modelType=\"ERM\", lins=\"r\"\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin_cao\"]),\n",
    "        modelType=\"Cao/Adaptive\",\n",
    "        lins=\"g\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin_eq\"]),\n",
    "        modelType=\"Tan/Equalised\",\n",
    "        lins=\"b\",\n",
    "    )\n",
    "    plot_db(\n",
    "        getwb(Xy_model_dict[config][\"trained_model_bin_la\"]),\n",
    "        modelType=\"Logit Adjusted\",\n",
    "        lins=\"purple\",\n",
    "    )\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\n",
    "        f\"Decision boundaries - {'Separable' if separable else 'Overlapping'}\",\n",
    "        fontsize=25,\n",
    "    )\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.grid()\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis([-4, 4, -4, 4])\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(8, 8)\n",
    "    plt.savefig(\n",
    "        f\"../data/{'imbal' if imbalanced else 'bal'}{'Sep' if separable else 'Overlap'}Margin.png\",\n",
    "        dpi=1200,\n",
    "    )\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
