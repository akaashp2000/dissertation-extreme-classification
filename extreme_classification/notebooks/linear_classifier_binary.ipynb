{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for generating plots for Chapter 4.1 (Experimental Results, Synthetic Data - Binary).\n",
    "\n",
    "It generates plots for decision boundaries of different linear binary classifiers for 2D data, learned from different loss functions.\n",
    "\n",
    "For both a) separable and b) overlapping and both i) balanced and ii) imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {\n",
    "    \"legend.fontsize\": \"x-large\",\n",
    "    \"axes.labelsize\": \"x-large\",\n",
    "    \"axes.titlesize\": \"x-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\")\n",
    "print(\"-\" * 60)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data directory, if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data\"):\n",
    "    os.makedirs(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "Plotting function, extract weights and biases from model, training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions\n",
    "\n",
    "* Get weights and biases from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwb(model, add: List[float] = [0, 0], mult: List[float] = [1, 1]):\n",
    "    \"\"\"Takes in binary LinearClassifier that has been fitted. Returns the weights and\n",
    "    biases of the two classes.\n",
    "\n",
    "    Args:\n",
    "        model (LinearClassifier): Linear Classifier\n",
    "        add (list, optional): Optional post-hoc addition to model's biases. Defaults to [0, 0].\n",
    "        mult (list, optional): Optional post-hoc scaling of each discriminant function. Defaults to [1, 1].\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The weights and biases of the model, post any offsets and scaling.\n",
    "    \"\"\"\n",
    "    _model = model.cpu()\n",
    "\n",
    "    return [\n",
    "        _model.linear.weight[0].detach().numpy() / mult[0],\n",
    "        _model.linear.weight[1].detach().numpy() / mult[1],\n",
    "        (_model.linear.bias[0].detach().numpy() + add[0]) / mult[0],\n",
    "        (_model.linear.bias[1].detach().numpy() + add[1]) / mult[1],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrain(\n",
    "    model, optimizer, criterion, X_train, y_train, adjust: torch.Tensor = None\n",
    "):\n",
    "    \"\"\"Trains a Pytorch model with an optimizer and criterion.\n",
    "    When evaluating the loss, an adjustment can be applied to the output of the model.\n",
    "\n",
    "    Args:\n",
    "        model: Model to be trained.\n",
    "        optimizer: Pytorch optimizer.\n",
    "        criterion: Loss function\n",
    "        X_train: Training X dataset.\n",
    "        y_train: Training y dataset.\n",
    "        adjust (torch.Tensor, optional): Additive update to output of model. Defaults to 0.\n",
    "    \"\"\"\n",
    "    adjust = torch.Tensor([0]).to(device) if adjust is None else adjust\n",
    "    for _ in range(10000):\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output + adjust, y_train.view(-1))\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions\n",
    "\n",
    "* plot decision boundaries from weights and biases\n",
    "* plot binary 2D data\n",
    "* repeated plot formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_db(wb: List[float], modelType: str = \"[modelType]\", lins=\"r\"):\n",
    "    \"\"\"Plots decision boundary line where -4 <= x, y <= 4, given the weights\n",
    "    and biases of the discriminant functions for each class. Options for\n",
    "    plot title and linestyle.\n",
    "\n",
    "    Args:\n",
    "        wb (List[float]): List of weights and biases.\n",
    "        modelType (str, optional): Label for plot. Defaults to '[modelType]'.\n",
    "        lins (str, optional): Linestyle of plot. Defaults to 'r'.\n",
    "    \"\"\"\n",
    "\n",
    "    w0, w1, b0, b1 = wb[0], wb[1], wb[2], wb[3]\n",
    "    x0 = np.linspace(-4, 4, 100)\n",
    "    x1 = (b1 - b0 - (w0[0] - w1[0]) * x0) / (w0[1] - w1[1])\n",
    "    plt.plot(x0, x1, lins, label=f\"{modelType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_2d_data_plot(sampA, sampB, linsA=\"cx\", linsB=\".y\"):\n",
    "    \"\"\"Create a 2D scatter plot of binary data points from two classes.\n",
    "\n",
    "    Args:\n",
    "        sampA (array-like): Data points from the first class to be plotted. It should be\n",
    "            a 2D array where each row represents a data point, and the first column\n",
    "            corresponds to the x-coordinate, and the second column corresponds to the\n",
    "            y-coordinate.\n",
    "        sampB (array-like): Data points from the second class to be plotted. It should be\n",
    "            a 2D array with the same structure as `sampA`.\n",
    "        linsA (str, optional): A string specifying the line style and color for sampA.\n",
    "            Default is \"cx\" (cyan 'x' markers).\n",
    "        linsB (str, optional): A string specifying the line style and color for sampB.\n",
    "            Default is \".y\" (yellow '.' markers).\n",
    "    \"\"\"\n",
    "    plt.plot(sampA[:, 0], sampA[:, 1], linsA)\n",
    "    plt.plot(sampB[:, 0], sampB[:, 1], linsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_plot(model_plot_configs: List[dict]):\n",
    "    \"\"\"\n",
    "    Generate plots for a list of models based on their configurations.\n",
    "\n",
    "    Args:\n",
    "        model_plot_configs (List[dict]): A list of dictionaries, where each dictionary\n",
    "            contains two key-value pairs:\n",
    "            - \"model_config\": A dictionary containing the kwargs for getwb (i.e. the model, and any multiplicate or additive update)\n",
    "            - \"plot_config\": A dictionary containing the kwargs for plot_db (i.e. plot title and linestyle)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for model_plot_config in model_plot_configs:\n",
    "        model_config = model_plot_config[\"model_config\"]\n",
    "        plot_config = model_plot_config[\"plot_config\"]\n",
    "\n",
    "        plot_db(wb=getwb(**model_config), **plot_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_formatting(config: tuple, suffix: str):\n",
    "    \"\"\"Apply formatting to a matplotlib plot, set title, labels, and save the plot.\n",
    "\n",
    "    Args:\n",
    "        config (tuple): A tuple containing two Boolean values representing the plot\n",
    "            configuration:\n",
    "            - imbalanced: Whether the data is imbalanced (True) or balanced (False).\n",
    "            - separable: Whether the data is separable (True) or overlapping (False).\n",
    "        suffix (str): A suffix to be added to the saved plot file name.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\n",
    "        f\"Decision boundaries - {'Separable' if separable else 'Overlapping'}\",\n",
    "        fontsize=25,\n",
    "    )\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.grid()\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis([-4, 4, -4, 4])\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(8, 8)\n",
    "    plt.savefig(\n",
    "        f\"../data/{'imbal' if imbalanced else 'bal'}{'Sep' if separable else 'Overlap'}{suffix}.png\",\n",
    "        dpi=1200,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generation and visualisation\n",
    "\n",
    "10,000 samples\n",
    "\n",
    "$ (100p)\\% $ from Gaussian centered at \n",
    "$\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "-1\n",
    "\\end{bmatrix}$ \n",
    "with covariance $ \\lambda I_2 $ where $ I_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$; \n",
    "\n",
    "the remaining $ (100(1-p))\\% $ from Gaussian centered at $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}$  with covariance $ \\lambda I_2 $.\n",
    "\n",
    "For a) separable, $ \\lambda = 0.04 $, and for b) overlapping, $ \\lambda = 1 $.\n",
    "\n",
    "For i) balanced, $ p = 0.5 $, and for ii) imbalanced, $ p = 0.95 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dictionary will have \n",
    "* key = (imbalanced: boolean, separable) and \n",
    "* value = {\n",
    "    \"class_prob\": list(float),\n",
    "    \"class_freq\": list(int),\n",
    "    \"sampA\": np.ndarray,\n",
    "    \"sampB\": np.ndarray,\n",
    "    \"X_train\": torch.Tensor, \n",
    "    \"y_train\": torch.Tensor,\n",
    "}\n",
    "class_prob is the list of class probabilities (length 2). Depends on imbalanced.\n",
    "class_freq is the list of class frequencies (length 2). I.e. class_prob * |training set|. \n",
    "sampA is the data for the first class ('negative', around (-1,-1)). Variance of the features depends on separable.\n",
    "sampB is the data for the second class ('positive', around (+1,+1)). Variance of the features depends on separable.\n",
    "X_train, y_train are the features and labels from sampA and sampB concatenated, stored as torch Tensors.\n",
    "\n",
    "Later the models for each dataset are added to the nested dictionaries.\n",
    "\"\"\"\n",
    "\n",
    "Xy_model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "for imbalanced, separable in list(itertools.product(*[[True, False]] * 2)):\n",
    "    config = (imbalanced, separable)\n",
    "    Xy_model_dict[config] = {}\n",
    "\n",
    "    Xy_model_dict[config][\"class_prob\"] = [0.95, 0.05] if imbalanced else [0.5, 0.5]\n",
    "    Xy_model_dict[config][\"class_freq\"] = [\n",
    "        int(N * i) for i in Xy_model_dict[config][\"class_prob\"]\n",
    "    ]\n",
    "    λ = 0.04 if separable else 1\n",
    "\n",
    "    covariance = λ * np.identity(2)\n",
    "    np.random.seed(2022)  # fix seed for a reproducable training set\n",
    "    Xy_model_dict[config][\"sampA\"] = np.random.multivariate_normal(\n",
    "        [-1, -1], covariance, Xy_model_dict[config][\"class_freq\"][0]\n",
    "    )\n",
    "    Xy_model_dict[config][\"sampB\"] = np.random.multivariate_normal(\n",
    "        [1, 1], covariance, Xy_model_dict[config][\"class_freq\"][1]\n",
    "    )\n",
    "    np.random.seed()\n",
    "\n",
    "    X_values = np.concatenate(\n",
    "        (Xy_model_dict[config][\"sampA\"], Xy_model_dict[config][\"sampB\"]), axis=0\n",
    "    )  # N-by-2 data matrix\n",
    "    y_flat = np.array(\n",
    "        [0] * Xy_model_dict[config][\"class_freq\"][0]\n",
    "        + [1] * Xy_model_dict[config][\"class_freq\"][1]\n",
    "    )  # row vector of labels\n",
    "\n",
    "    index_shuffle = np.arange(len(X_values))\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    # shuffle the rows of data matrix and labels\n",
    "\n",
    "    X_values = X_values[index_shuffle]\n",
    "    y_flat = y_flat[index_shuffle]\n",
    "\n",
    "    # convert to Torch tensors\n",
    "\n",
    "    X_train = torch.tensor(X_values).type(torch.FloatTensor).to(device)\n",
    "    y_train = torch.from_numpy(y_flat).view(-1, 1).type(torch.LongTensor).to(device)\n",
    "\n",
    "    Xy_model_dict[config] |= {\"X_train\": X_train, \"y_train\": y_train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier\n",
    "\n",
    "A very simple linear neural network with input dimension 2 and output dimension 2 (the discriminant function for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a base LinearClassifier. We make copies of this and train with different criteria and updates.\n",
    "\n",
    "model_bin_orig = LinearClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard classifier\n",
    "\n",
    "With single layer NN with Cross-Entropy Loss (for binary classification, this is equivalent to a sigmoid classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin = torch.optim.SGD(model_bin.parameters(), lr=0.1, weight_decay=0)\n",
    "\n",
    "    criterion_bin = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin\"] = modelTrain(\n",
    "        model_bin,\n",
    "        optimizer_bin,\n",
    "        criterion_bin,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss modified\n",
    "\n",
    "Balance the loss, by weighting it by: \n",
    "* the inverse of the class probability\n",
    "* the inverse of the (Cui et. al) effective class frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_bal = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin_bal = torch.optim.SGD(\n",
    "        model_bin_bal.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "\n",
    "    criterion_bin_bal = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(np.reciprocal(Xy_model_dict[config][\"class_prob\"])).type(\n",
    "            torch.FloatTensor\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin_bal\"] = modelTrain(\n",
    "        model_bin_bal,\n",
    "        optimizer_bin_bal,\n",
    "        criterion_bin_bal,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999\n",
    "\n",
    "for config in Xy_model_dict.keys():\n",
    "    cui_weights = [\n",
    "        (1 - beta) / (1 - beta**i) for i in Xy_model_dict[config][\"class_freq\"]\n",
    "    ]  # inverse effective number for each class\n",
    "    cui_weights = [i / sum(cui_weights) for i in cui_weights]  # normalise weights\n",
    "    cui_weights = torch.tensor(cui_weights).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    model_bin_cui = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin_cui = torch.optim.SGD(\n",
    "        model_bin_cui.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "\n",
    "    criterion_bin_cui = nn.CrossEntropyLoss(weight=cui_weights).to(device)\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin_cui\"] = modelTrain(\n",
    "        model_bin_cui,\n",
    "        optimizer_bin_cui,\n",
    "        criterion_bin_cui,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cao's LDAM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAM Loss taken from Cao et. al code and modified\n",
    "\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        delta = 1.0 / np.sqrt(np.sqrt(class_prob))\n",
    "        delta = torch.FloatTensor(delta).to(device)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8).to(device)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        index_bool = index == 1\n",
    "        index_float = index.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = torch.matmul(self.delta[None, :], index_float.transpose(0, 1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "\n",
    "        # computes logits modified by subtracting the margin for each example (determined by the true class) from the each output for it\n",
    "        outputs_m = outputs - batch_m\n",
    "\n",
    "        # for each example, replace the logit for the true class by the modified one, and keep the others the same\n",
    "        output = torch.where(index_bool, outputs_m, outputs)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_cao = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin_cao = torch.optim.SGD(\n",
    "        model_bin_cao.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "\n",
    "    criterion_bin_cao = LDAMLoss(Xy_model_dict[config][\"class_prob\"])\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin_cao\"] = modelTrain(\n",
    "        model_bin_cao,\n",
    "        optimizer_bin_cao,\n",
    "        criterion_bin_cao,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalised Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss with margin log(pi_j) for where j is the negative class\n",
    "\n",
    "\n",
    "class EqLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(EqLoss, self).__init__()\n",
    "        delta = np.log(class_prob)\n",
    "        delta = torch.FloatTensor(delta).to(device)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        # one-hot encodes the binary labels\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8).to(device)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        index_bool = index == 1\n",
    "\n",
    "        # column vector of the margin adjustments\n",
    "        batch_m = self.delta.repeat(outputs.shape[0], 1)\n",
    "\n",
    "        # computes logits modified by adding the margin, log(pi_j), to each logit f_j for each example\n",
    "        outputs_m = outputs + batch_m\n",
    "\n",
    "        # for each example, replace the logit for the negative classes by the modified one in outputs_m, and keep original logit for true class\n",
    "        output = torch.where(index_bool, outputs, outputs_m)\n",
    "\n",
    "        # computes the mean of cross-entropy losses for each example and its adjusted output\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    model_bin_eq = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin_eq = torch.optim.SGD(\n",
    "        model_bin_eq.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "\n",
    "    criterion_bin_eq = EqLoss(Xy_model_dict[config][\"class_prob\"])\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin_eq\"] = modelTrain(\n",
    "        model_bin_eq,\n",
    "        optimizer_bin_eq,\n",
    "        criterion_bin_eq,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit-Adjusted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    la_margin_mat = (\n",
    "        torch.Tensor(np.array([np.log(Xy_model_dict[config][\"class_prob\"])] * N))\n",
    "        .type(torch.float32)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    model_bin_la = deepcopy(model_bin_orig)\n",
    "\n",
    "    optimizer_bin_la = torch.optim.SGD(\n",
    "        model_bin_la.parameters(), lr=0.1, weight_decay=0\n",
    "    )\n",
    "\n",
    "    criterion_bin_la = nn.CrossEntropyLoss()\n",
    "\n",
    "    Xy_model_dict[config][\"trained_model_bin_la\"] = modelTrain(\n",
    "        model_bin_la,\n",
    "        optimizer_bin_la,\n",
    "        criterion_bin_la,\n",
    "        Xy_model_dict[config][\"X_train\"],\n",
    "        Xy_model_dict[config][\"y_train\"],\n",
    "        adjust=la_margin_mat,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the decision boundaries\n",
    "\n",
    "Decision boundaries for each linear classifier (Vanilla ERM, logit-adjusted, reweighted and modified loss).\n",
    "\n",
    "We also include the Bayes classifier decision boundaries (y = - x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    # Plot the decision boundaries - can plot boundaries for different models\n",
    "\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    binary_2d_data_plot(\n",
    "        sampA=Xy_model_dict[config][\"sampA\"],\n",
    "        sampB=Xy_model_dict[config][\"sampB\"],\n",
    "        linsA=\"cx\",\n",
    "        linsB=\".y\",\n",
    "    )\n",
    "\n",
    "    models_to_plot(\n",
    "        [\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"β = 0\", \"lins\": \"r\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin_cui\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": f\"β = {beta}\", \"lins\": \"b\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin_bal\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"β = 1\", \"lins\": \"g\"},\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "\n",
    "    plot_formatting(config=config, suffix=\"Weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    binary_2d_data_plot(\n",
    "        sampA=Xy_model_dict[config][\"sampA\"],\n",
    "        sampB=Xy_model_dict[config][\"sampB\"],\n",
    "        linsA=\"cx\",\n",
    "        linsB=\".y\",\n",
    "    )\n",
    "\n",
    "    models_to_plot(\n",
    "        [\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin\"],\n",
    "                    \"mult\": Xy_model_dict[config][\"class_prob\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"Multiplicative update\", \"lins\": \"g\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin\"],\n",
    "                    \"add\": -np.log(Xy_model_dict[config][\"class_prob\"]),\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"Additive update\", \"lins\": \"b\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"ERM\", \"lins\": \"r\"},\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "\n",
    "    plot_formatting(config=config, suffix=\"Update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin adjusted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in Xy_model_dict.keys():\n",
    "    imbalanced, separable = config\n",
    "\n",
    "    binary_2d_data_plot(\n",
    "        sampA=Xy_model_dict[config][\"sampA\"],\n",
    "        sampB=Xy_model_dict[config][\"sampB\"],\n",
    "        linsA=\"cx\",\n",
    "        linsB=\".y\",\n",
    "    )\n",
    "\n",
    "    models_to_plot(\n",
    "        [\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"ERM\", \"lins\": \"r\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin_cao\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"Cao/Adaptive\", \"lins\": \"g\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin_eq\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"Tan/Equalised\", \"lins\": \"b\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_config\": {\n",
    "                    \"model\": Xy_model_dict[config][\"trained_model_bin_la\"],\n",
    "                },\n",
    "                \"plot_config\": {\"modelType\": \"Logit Adjusted\", \"lins\": \"purple\"},\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    plot_db([[0, 0], [1, 1], 0, 0], modelType=\"Bayes classifier\", lins=\"k--\")\n",
    "\n",
    "    plot_formatting(config=config, suffix=\"Margin\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
