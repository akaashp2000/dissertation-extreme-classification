{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for simple Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a function that has input a binary linear classifier's weights and biases, and outputs the decision boundary line in [-4,4] * [-4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_db(w0, w1, b0, b1, modelType='[modelType]', lins='r'):\n",
    "\n",
    "    x0 = np.linspace(-4,4,100)\n",
    "    x1 = (b1 - b0 - (w0[0] - w1[0]) * x0) / (w0[1] - w1[1])\n",
    "    plt.plot(x0, x1, lins, label=f'{modelType}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nxrS5pU3Tb8"
   },
   "source": [
    "### Synthetic data generation and visualisation\n",
    "\n",
    "10,000 samples - 95% from Gaussian centered at (-1, -1) with covariance I; the remaining 5% from Gaussian centered at (1, 1) with covariance I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9LpO2hYuGvz"
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "class_prob = [0.95, 0.05]\n",
    "class_freq = [int(N * i) for i in class_prob]\n",
    "\n",
    "covariance = np.identity(2) * 0.04\n",
    "np.random.seed(2022)\n",
    "sampA = np.random.multivariate_normal([-1,-1], covariance, class_freq[0])\n",
    "sampB = np.random.multivariate_normal([1,1], covariance, class_freq[1])\n",
    "np.random.seed()\n",
    "\n",
    "X_values = np.concatenate((sampA, sampB), axis=0)\n",
    "y_flat = np.array([0] * class_freq[0] + [1] * class_freq[1])\n",
    "\n",
    "index_shuffle = np.arange(len(X_values))\n",
    "np.random.shuffle(index_shuffle)\n",
    "\n",
    "X_values = X_values[index_shuffle]\n",
    "y_flat = y_flat[index_shuffle]\n",
    "\n",
    "X_train = torch.tensor(X_values).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_flat).view(-1,1).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the standard classifier \n",
    "\n",
    "With single layer NN with Cross-Entropy Loss (for binary classification equivalent to a sigmoid classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_orig = LinearClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = deepcopy(model_bin_orig)\n",
    "optimizer_bin = torch.optim.SGD(model_bin.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrain(model, optimizer, criterion):\n",
    "    for epoch in range(10000):\n",
    "    \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train.view(-1))\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin, optimizer_bin, criterion_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = model_bin.linear.weight[0].detach().numpy()\n",
    "w1 = model_bin.linear.weight[1].detach().numpy()\n",
    "b0 = model_bin.linear.bias[0].detach().numpy()\n",
    "b1 = model_bin.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweighted logits\n",
    "\n",
    "Multiplicative update - by class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_weighted = w0/class_prob[0]\n",
    "w1_weighted = w1/class_prob[1]\n",
    "b0_weighted = b0/class_prob[0]\n",
    "b1_weighted = b1/class_prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted logits\n",
    "\n",
    "Additive update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_adjusted = b0 - np.log(class_prob[0])\n",
    "b1_adjusted = b1 - np.log(class_prob[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss modified\n",
    "\n",
    "Balance the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_bal = deepcopy(model_bin_orig)\n",
    "optimizer_bin_bal = torch.optim.SGD(model_bin_bal.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_bal = torch.nn.CrossEntropyLoss(weight = torch.tensor(np.reciprocal(class_prob)).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_bal, optimizer_bin_bal, criterion_bin_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999\n",
    "cui_weights = [(1 - beta) / (1 - beta ** i) for i in class_freq]\n",
    "cui_weights = [i / sum(cui_weights) for i in cui_weights]\n",
    "cui_weights = torch.tensor(cui_weights).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_cui = deepcopy(model_bin_orig)\n",
    "optimizer_bin_cui = torch.optim.SGD(model_bin_cui.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_cui = torch.nn.CrossEntropyLoss(weight = cui_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain(model_bin_cui, optimizer_bin_cui, criterion_bin_cui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cao's LDAM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAMLoss(torch.nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        delta = 1.0 / np.sqrt(np.sqrt(class_prob))\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        index_float = index.type(torch.FloatTensor)\n",
    "        batch_m = torch.matmul(self.delta[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "        outputs_m = outputs - batch_m\n",
    "    \n",
    "        output = torch.where(index, outputs_m, outputs)\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_cao = deepcopy(model_bin_orig)\n",
    "optimizer_bin_cao = torch.optim.SGD(model_bin_cao.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_cao = LDAMLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin_cao = []\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin_cao.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin_cao = model_bin_cao(X_train)\n",
    "    loss_bin_cao = criterion_bin_cao(output_bin_cao, y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin_cao.append(loss_bin_cao.item())\n",
    "    loss_bin_cao.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin_cao.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_cao = model_bin_cao.linear.weight[0].detach().numpy()\n",
    "w1_cao = model_bin_cao.linear.weight[1].detach().numpy()\n",
    "b0_cao = model_bin_cao.linear.bias[0].detach().numpy()\n",
    "b1_cao = model_bin_cao.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tan's Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None):\n",
    "        super(TanLoss, self).__init__()\n",
    "        delta = np.log(class_prob)\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        batch_m = self.delta.repeat(outputs.shape[0],1)\n",
    "        outputs_m = outputs + batch_m\n",
    "    \n",
    "        output = torch.where(index, outputs, outputs_m)\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_tan = deepcopy(model_bin_orig)\n",
    "optimizer_bin_tan = torch.optim.SGD(model_bin_tan.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_tan = TanLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin_tan = []\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin_tan.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin_tan = model_bin_tan(X_train)\n",
    "    loss_bin_tan = criterion_bin_tan(output_bin_tan, y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin_tan.append(loss_bin_tan.item())\n",
    "    loss_bin_tan.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin_tan.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_tan = model_bin_tan.linear.weight[0].detach().numpy()\n",
    "w1_tan = model_bin_tan.linear.weight[1].detach().numpy()\n",
    "b0_tan = model_bin_tan.linear.bias[0].detach().numpy()\n",
    "b1_tan = model_bin_tan.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit-Adjusted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin_la = deepcopy(model_bin_orig)\n",
    "optimizer_bin_la = torch.optim.SGD(model_bin_la.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_la = torch.nn.CrossEntropyLoss()\n",
    "la_margin_mat = torch.Tensor(np.array([np.log(class_prob) for _ in range(N)])).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin_la = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin_la.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin_la = model_bin_la(X_train)\n",
    "    loss_bin_la = criterion_bin_la(torch.add(output_bin_la, la_margin_mat), y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin_la.append(loss_bin_la.item())\n",
    "    loss_bin_la.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin_la.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_la = model_bin_la.linear.weight[0].detach().numpy()\n",
    "w1_la = model_bin_la.linear.weight[1].detach().numpy()\n",
    "b0_la = model_bin_la.linear.bias[0].detach().numpy()\n",
    "b1_la = model_bin_la.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundaries\n",
    "\n",
    "Decision boundaries for each linear classifier (Vanilla ERM, logit-adjusted, reweighted and modified loss).\n",
    "\n",
    "We also include the Bayes classifier decision boundaries (y = - x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_bal = model_bin_bal.linear.weight[0].detach().numpy()\n",
    "w1_bal = model_bin_bal.linear.weight[1].detach().numpy()\n",
    "b0_bal = model_bin_bal.linear.bias[0].detach().numpy()\n",
    "b1_bal = model_bin_bal.linear.bias[1].detach().numpy()\n",
    "\n",
    "w0_cui = model_bin_cui.linear.weight[0].detach().numpy()\n",
    "w1_cui = model_bin_cui.linear.weight[1].detach().numpy()\n",
    "b0_cui = model_bin_cui.linear.bias[0].detach().numpy()\n",
    "b1_cui = model_bin_cui.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plot_db(w0, w1, b0, b1, modelType='β = 0', lins='r')\n",
    "plot_db(w0_cui, w1_cui, b0_cui, b1_cui, modelType=f'β = {beta}', lins='b')\n",
    "plot_db(w0_bal, w1_bal, b0_bal, b1_bal, modelType='β = 1', lins='g')\n",
    "plot_db([0,0], [1,1], 0, 0, modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Separable', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalSepWeighted.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit adjusted decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plot_db(w0, w1, b0, b1, modelType='ERM', lins='r')\n",
    "plot_db(w0, w1, b0_adjusted, b1_adjusted, modelType='Additive update', lins='b')\n",
    "plot_db(w0_weighted, w1_weighted, b0_weighted, b1_weighted, modelType='Multiplicative update', lins='g')\n",
    "plot_db([0,0], [1,1], 0, 0, modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Separable', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalSepUpdate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin adjusted loss decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plot_db(w0, w1, b0, b1, modelType='ERM', lins='r')\n",
    "plot_db(w0_cao, w1_cao, b0_cao, b1_cao, modelType='LDAM', lins='g')\n",
    "plot_db(w0_tan, w1_tan, b0_tan, b1_tan, modelType='Equalised', lins='b')\n",
    "plot_db(w0_la, w1_la, b0_la, b1_la, modelType='Logit-adjusted', lins='purple')\n",
    "plot_db([0,0], [1,1], 0, 0, modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Decision boundaries - Separable', fontsize=25)\n",
    "plt.xlabel('x1', fontsize=20)\n",
    "plt.ylabel('x2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('imbalSepMargin.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6dc7853e01262b9d99422a25edd5bb7869d5d1bdf17c6d04e3e60055b23b476"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
