{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for simple Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a function that has input a binary linear classifier's weights and biases, and outputs the decision boundary line in [-4,4] * [-4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_db(w0, w1, b0, b1, modelType='[modelType]', lins='r'):\n",
    "\n",
    "    x0 = np.linspace(-4,4,100)\n",
    "    x1 = (b1 - b0 - (w0[0] - w1[0]) * x0) / (w0[1] - w1[1])\n",
    "    plt.plot(x0, x1, lins, label=f'{modelType}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nxrS5pU3Tb8"
   },
   "source": [
    "### Synthetic data generation and visualisation\n",
    "\n",
    "10,000 samples - 95% from Gaussian centered at (-1, -1) with covariance I; the remaining 5% from Gaussian centered at (1, 1) with covariance I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9LpO2hYuGvz"
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "class_prob = [0.95, 0.05]\n",
    "class_freq = [int(N * i) for i in class_prob]\n",
    "\n",
    "covariance = np.identity(2) * 1\n",
    "sampA = np.random.multivariate_normal([-1,-1], covariance, class_freq[0])\n",
    "sampB = np.random.multivariate_normal([1,1], covariance, class_freq[1])\n",
    "\n",
    "X_values = np.concatenate((sampA, sampB), axis=0)\n",
    "y_flat = np.array([0] * class_freq[0] + [1] * class_freq[1])\n",
    "\n",
    "index_shuffle = np.arange(len(X_values))\n",
    "np.random.shuffle(index_shuffle)\n",
    "\n",
    "X_values = X_values[index_shuffle]\n",
    "y_flat = y_flat[index_shuffle]\n",
    "\n",
    "X_train = torch.tensor(X_values).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_flat).view(-1,1).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Generated data by class')\n",
    "\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "\n",
    "plt.plot(sampA[:,0], sampA[:,1], 'bx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the standard classifier \n",
    "\n",
    "With single layer NN with Cross-Entropy Loss (for binary classification equivalent to a sigmoid classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = LinearClassifier()\n",
    "model_bin_cao = deepcopy(model_bin)\n",
    "model_bin_tan = deepcopy(model_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bin = torch.optim.SGD(model_bin.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin = []\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin = model_bin(X_train)\n",
    "    loss_bin = criterion_bin(output_bin, y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin.append(loss_bin.item())\n",
    "    loss_bin.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = model_bin.linear.weight[0].detach().numpy()\n",
    "w1 = model_bin.linear.weight[1].detach().numpy()\n",
    "b0 = model_bin.linear.bias[0].detach().numpy()\n",
    "b1 = model_bin.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cao's LDAM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAMLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None, s=1):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        delta = 1.0 / np.sqrt(np.sqrt(class_prob))\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        assert s > 0\n",
    "        self.s = s\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        index_float = index.type(torch.FloatTensor)\n",
    "        batch_m = torch.matmul(self.delta[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "        outputs_m = outputs - batch_m\n",
    "    \n",
    "        output = torch.where(index, outputs_m, outputs)\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bin_cao = torch.optim.SGD(model_bin_cao.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_cao = LDAMLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin_cao = []\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin_cao.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin_cao = model_bin_cao(X_train)\n",
    "    loss_bin_cao = criterion_bin_cao(output_bin_cao, y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin_cao.append(loss_bin_cao.item())\n",
    "    loss_bin_cao.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin_cao.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_cao = model_bin_cao.linear.weight[0].detach().numpy()\n",
    "w1_cao = model_bin_cao.linear.weight[1].detach().numpy()\n",
    "b0_cao = model_bin_cao.linear.bias[0].detach().numpy()\n",
    "b1_cao = model_bin_cao.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tan's Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanLoss(nn.Module):\n",
    "    def __init__(self, class_prob, weight=None, s=1):\n",
    "        super(TanLoss, self).__init__()\n",
    "        delta = np.log(class_prob)\n",
    "        delta = torch.FloatTensor(delta)\n",
    "        self.delta = delta\n",
    "        assert s > 0\n",
    "        self.s = s\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        index = torch.zeros_like(outputs, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        batch_m = self.delta.repeat(outputs.shape[0],1)\n",
    "        outputs_m = outputs + batch_m\n",
    "    \n",
    "        output = torch.where(index, outputs, outputs_m)\n",
    "        return F.cross_entropy(output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bin_tan = torch.optim.SGD(model_bin_tan.parameters(), lr=0.1, weight_decay=0)\n",
    "criterion_bin_tan = TanLoss(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_bin_tan = []\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # zero the gradients\n",
    "    optimizer_bin_tan.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    output_bin_tan = model_bin_tan(X_train)\n",
    "    loss_bin_tan = criterion_bin_tan(output_bin_tan, y_train.view(-1))\n",
    "\n",
    "    # backward\n",
    "    all_loss_bin_tan.append(loss_bin_tan.item())\n",
    "    loss_bin_tan.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer_bin_tan.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_tan = model_bin_tan.linear.weight[0].detach().numpy()\n",
    "w1_tan = model_bin_tan.linear.weight[1].detach().numpy()\n",
    "b0_tan = model_bin_tan.linear.bias[0].detach().numpy()\n",
    "b1_tan = model_bin_tan.linear.bias[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundaries\n",
    "\n",
    "Decision boundaries for each linear classifier (Vanilla ERM, logit-adjusted, reweighted and modified loss).\n",
    "\n",
    "We also include the Bayes classifier decision boundaries (y = - x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampA[:,0], sampA[:,1], 'cx')\n",
    "plt.plot(sampB[:,0], sampB[:,1], '.y')\n",
    "plot_db(w0, w1, b0, b1, modelType='ERM', lins='r')\n",
    "plot_db(w0_cao, w1_cao, b0_cao, b1_cao, modelType='Cao/Adaptive', lins='g')\n",
    "plot_db(w0_tan, w1_tan, b0_tan, b1_tan, modelType='Tan/Equalised', lins='b')\n",
    "plot_db([0,0], [1,1], 0, 0, modelType='Bayes classifier', lins='k--')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Decision boundaries - Overlapping')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.grid()\n",
    "plt.axis('scaled')\n",
    "plt.axis([-4,4,-4,4])\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,8)\n",
    "plt.savefig('../data/imbalOverlapWeighted.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6dc7853e01262b9d99422a25edd5bb7869d5d1bdf17c6d04e3e60055b23b476"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
