{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "from utils import download_tfrecord\n",
    "from tensorflow import make_ndarray\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from tensorboard.backend.event_processing.tag_types import TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a TensorBoard Session in VS Code or...\n",
    "\n",
    "Run the below (you may have to run the second command twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use\n",
    "\n",
    "```\n",
    "!tensorboard --logdir log\n",
    "```\n",
    "if the above doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Run the below cells to download the CIFAR-10 datasets (or the CIFAR-100 ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_tfrecord(\"test10\", \"train10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (and evaluation)\n",
    "\n",
    "Use ```main.py``` to train on the imbalanced version of CIFAR 10. This is to get the results in Table 1.\n",
    "\n",
    "Call it with the following flags/parameters:\n",
    "\n",
    "* ```dataset```: ```cifar10-lt``` (for CIFAR 10)\n",
    "* ```mode```: ```baseline```, ```posthoc``` or ```loss```\n",
    "    * whether to use baseline (vanilla) ERM\n",
    "    * or posthoc modification of logits\n",
    "    * or adjusted loss function\n",
    "\n",
    "The results can be found in Tensorboard: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --dataset=cifar10-lt --mode=baseline --tb_log_dir=log/ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posthoc update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --dataset=cifar10-lt --mode=posthoc --tb_log_dir=log/Additive_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit adjusted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --dataset=cifar10-lt --mode=loss --tb_log_dir=log/LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the results\n",
    "\n",
    "* For Figure 1 and Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class probabilities\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"./data/cifar10-lt_base_probs.txt\"\n",
    "\n",
    "# Initialize a list to store the numbers\n",
    "class_prob = []\n",
    "\n",
    "# Open the file and read it line by line\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Remove leading/trailing whitespace and convert the line to a float\n",
    "        number = float(line.strip())\n",
    "        class_prob.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of samples in the training data. possible values for β\n",
    "\n",
    "N = 10000\n",
    "βs = [1, 0.999, 0.99, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate effective class frequency with formula in terms of β and empirical class frequency\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"class_num\": list(range(1, 10 + 1)) * len(βs),\n",
    "        \"class_prob\": class_prob * len(βs),\n",
    "        \"β\": sum([[β] * 10 for β in βs], start=[]),\n",
    "    }\n",
    ")\n",
    "\n",
    "df[\"effective_class_frequency\"] = (1 - df[\"β\"] ** (N * df[\"class_prob\"])) / (\n",
    "    1 - df[\"β\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise effective class frequency to get effective class probability\n",
    "\n",
    "df[\"effective_class_frequency\"] = df[\"effective_class_frequency\"] / df.groupby(\"β\")[\n",
    "    \"effective_class_frequency\"\n",
    "].transform(\"sum\")\n",
    "\n",
    "# when β = 1, use empirical class probability\n",
    "\n",
    "df[\"effective_class_frequency\"] = np.where(\n",
    "    df[\"effective_class_frequency\"].isna(),\n",
    "    df[\"class_prob\"],\n",
    "    df[\"effective_class_frequency\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot effective class probability for each class number for each β\n",
    "\n",
    "alt.Chart(data=df).mark_bar().encode(\n",
    "    x=alt.X(\"β:N\", title=None, axis=alt.Axis(ticks=False, labels=False), sort=βs),\n",
    "    y=alt.Y(\"effective_class_frequency\", title=\"Smoothed class probability\"),\n",
    "    color=alt.Color(\"β:N\", sort=βs),\n",
    "    column=alt.Column(\"class_num:O\", title=\"Classes (ranked by frequency)\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1\n",
    "\n",
    "We get the final test accuracy for each of the approaches (baseline ERM, posthoc update and logit adjusted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensorboard_data(log_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the logs from Tensorboard SummaryWriter in blob storage as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): the path to the Tensorboard logs\n",
    "        print_scalars (bool): whether or not to print the list of scalars from the EventAccumulator. By default False (so doesn't print).\n",
    "    \"\"\"\n",
    "\n",
    "    event_acc = EventAccumulator(log_dir, size_guidance={TENSORS: 0})\n",
    "    event_acc.Reload()\n",
    "\n",
    "    data = []\n",
    "    tensors = event_acc.Tags()[\"tensors\"]\n",
    "\n",
    "    for tag in tensors:\n",
    "        events = event_acc.Tensors(tag)\n",
    "        steps = [event.step for event in events]\n",
    "        tensor_protos = [make_ndarray(event.tensor_proto).item() for event in events]\n",
    "        for step, tensor_proto in list(zip(steps, tensor_protos)):\n",
    "            data.append(\n",
    "                {\n",
    "                    \"experiment\": exp_name,\n",
    "                    \"metric\": tag,\n",
    "                    \"step\": step,\n",
    "                    \"value\": tensor_proto,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_names = sorted(os.listdir(\"./log\"))  # the folder names\n",
    "exp_names = [\n",
    "    _exp_name.replace(\"_\", \" \") for _exp_name in _exp_names\n",
    "]  # remove underscore for space\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for _exp_name, exp_name in list(zip(_exp_names, exp_names)):\n",
    "    df = pd.concat([df, load_tensorboard_data(\"./log/\" + _exp_name + \"/test\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy_df = df[df.step == 19200].reset_index().drop([\"index\", \"step\"], axis=1)\n",
    "final_accuracy_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
